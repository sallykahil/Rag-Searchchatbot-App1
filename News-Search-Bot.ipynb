{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec0895f4",
   "metadata": {},
   "source": [
    "### RAG GOld and silver News Search Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c363ef90",
   "metadata": {},
   "source": [
    "files that will be used in  \n",
    "https://pubs.usgs.gov/periodicals/mcs2025/mcs2025-gold.pdf\n",
    "https://pubs.usgs.gov/periodicals/mcs2025/mcs2025-silver.pdf\n",
    "https://silverinstitute.org/wp-content/uploads/2025/04/World_Silver_Survey-2025.pdf\n",
    "https://thedocs.worldbank.org/en/doc/718001587677508339-0050022020/render/CMOApril2020preciousmetals.txt\n",
    "https://github.com/datasets/gold-prices/raw/main/data/monthly.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "653361fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === URL fetching ===\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# === Document loading (PDF with tables + TXT) ===\n",
    "from langchain_community.document_loaders import PDFPlumberLoader, TextLoader, CSVLoader\n",
    "# Alternative for simple PDFs (no tables): PyPDFLoader\n",
    "\n",
    "# === Chunking ===\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# === Embeddings ===\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # or HuggingFaceBgeEmbeddings\n",
    "\n",
    "# === Vector store ===\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# === LLM + RAG chain ===\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "from langchain_classic.prompts import PromptTemplate\n",
    "# + your LLM: HuggingFacePipeline, or Ollama, or ChatOpenAI\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "from langchain_classic.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cf4379",
   "metadata": {},
   "source": [
    "### 1-load documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9692d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs and download folder\n",
    "URLS = [\n",
    "    \"https://pubs.usgs.gov/periodicals/mcs2025/mcs2025-gold.pdf\",\n",
    "    \"https://pubs.usgs.gov/periodicals/mcs2025/mcs2025-silver.pdf\",\n",
    "    \"https://silverinstitute.org/wp-content/uploads/2025/04/World_Silver_Survey-2025.pdf\",\n",
    "    \"https://thedocs.worldbank.org/en/doc/718001587677508339-0050022020/render/CMOApril2020preciousmetals.txt\",\n",
    "    \"https://github.com/datasets/gold-prices/raw/main/data/monthly.csv\",\n",
    "]\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "621a89c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: data\\mcs2025-gold.pdf\n",
      "Downloaded: data\\mcs2025-silver.pdf\n",
      "Downloaded: data\\World_Silver_Survey-2025.pdf\n",
      "Downloaded: data\\CMOApril2020preciousmetals.txt\n",
      "Downloaded: data\\monthly.csv\n"
     ]
    }
   ],
   "source": [
    "def download_file(url: str, save_dir: str) -> str:\n",
    "    \"\"\"Download file from URL and return local path. Writes binary to preserve encoding.\"\"\"\n",
    "    filename = url.rpartition(\"/\")[-1].split(\"?\")[0]\n",
    "    filepath = os.path.join(save_dir, filename)\n",
    "    response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    return filepath\n",
    "\n",
    "\n",
    "downloaded = []\n",
    "for url in URLS:\n",
    "    try:\n",
    "        path = download_file(url, DATA_DIR)\n",
    "        downloaded.append(path)\n",
    "        print(f\"Downloaded: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed {url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02d8458c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\mcs2025-gold.pdf: 743,291 bytes\n",
      "data\\mcs2025-silver.pdf: 744,783 bytes\n",
      "data\\World_Silver_Survey-2025.pdf: 12,951,882 bytes\n",
      "data\\CMOApril2020preciousmetals.txt: 26,324 bytes\n",
      "data\\monthly.csv: 35,492 bytes\n"
     ]
    }
   ],
   "source": [
    "# Verify downloads\n",
    "for p in downloaded:\n",
    "    size = os.path.getsize(p) if os.path.exists(p) else 0\n",
    "    print(f\"{p}: {size:,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccc9ecdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 doc(s) from data\\mcs2025-gold.pdf\n",
      "Loaded 2 doc(s) from data\\mcs2025-silver.pdf\n",
      "Loaded 92 doc(s) from data\\World_Silver_Survey-2025.pdf\n",
      "Loaded 1 doc(s) from data\\CMOApril2020preciousmetals.txt\n",
      "Loaded 2311 doc(s) from data\\monthly.csv\n"
     ]
    }
   ],
   "source": [
    "# Load all documents\n",
    "all_docs = []\n",
    "\n",
    "for path in downloaded:\n",
    "    if not os.path.exists(path):\n",
    "        continue\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    try:\n",
    "        if ext == \".pdf\":\n",
    "            loader = PDFPlumberLoader(path)\n",
    "        elif ext == \".csv\":\n",
    "            loader = CSVLoader(path)\n",
    "        else:\n",
    "            loader = TextLoader(path, encoding=\"utf-8\", autodetect_encoding=True)\n",
    "        docs = loader.load()\n",
    "        all_docs.extend(docs)\n",
    "        print(f\"Loaded {len(docs)} doc(s) from {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0ab3565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 2744\n",
      "Sample chunk (627 chars):\n",
      "82\n",
      "GOLD\n",
      "(Data in metric tons,1 gold content, unless otherwise specified)\n",
      "Domestic Production and Use: In 2024, domestic gold mine production was estimated to be 160 tons; the value\n",
      "was estimated to be $12 billion, a 9% increase from the value in 2023. Gold was produced at more than 40 lode\n",
      "mines in ...\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=700,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(all_docs)\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(f\"Sample chunk ({len(chunks[0].page_content)} chars):\\n{chunks[0].page_content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ab0c3d",
   "metadata": {},
   "source": [
    "## 2. Embed and Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "303eac8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_36832\\510061179.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings ready: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Embedding model (sentence-transformers under the hood)\n",
    "# all-MiniLM-L6-v2: fast, ~384 dims | all-mpnet-base-v2: better quality, slower\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBED_MODEL,\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")\n",
    "print(f\"Embeddings ready: {EMBED_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278dbe3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created and saved to faiss_index/ (2744 chunks)\n"
     ]
    }
   ],
   "source": [
    "# Create FAISS vector store from chunks\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# Optional: save to disk for reuse (no need to re-embed next time)\n",
    "#Why save it?\n",
    "# \tAvoid re-embedding and re-building the index every run, which can be time-consuming. You can load the saved index later to quickly restore the vector store without reprocessing the documents.\n",
    "FAISS_INDEX = \"faiss_index\"\n",
    "vectorstore.save_local(FAISS_INDEX)\n",
    "print(f\"Vector store created and saved to {FAISS_INDEX}/ ({len(chunks)} chunks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40100b8",
   "metadata": {},
   "source": [
    "## 3. Query & Retrieve + Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "285fad21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever ready (top-4 chunks)\n"
     ]
    }
   ],
   "source": [
    "# Retriever: embed question, search FAISS, return top-k chunks\n",
    "TOP_K = 4\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",  # \"similarity\" for pure relevance, \"mmr\" to balance relevance and diversity\n",
    "    search_kwargs={\"k\": TOP_K},\n",
    ")\n",
    "print(f\"Retriever ready (top-{TOP_K} chunks)\")\n",
    "# search_type=\"mmr\" â€“ balances relevance and diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "164cb269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM ready: google/flan-t5-small\n"
     ]
    }
   ],
   "source": [
    " \n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.3, \n",
    ")\n",
    "#pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=256)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "print(f\"LLM ready: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095d5d73",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    " \n",
    "prompt_template = \"\"\"Use the context below to answer ONLY the specific question asked. Do NOT use choice letters (A, B, 1, 2, 3, 4) or numbers alone\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015400bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain ready\n"
     ]
    }
   ],
   "source": [
    "# RAG chain: retriever + LLM\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    ")\n",
    "print(\"RAG chain ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c273027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 3.300 tons\n",
      "Answer: The estimated price in 2024 increased by 23% and reached a new record-high annual price compared with the previous record-high annual price in 2023.\n",
      "\n",
      "--- Source chunks (top-k) ---\n",
      "\n",
      "[1] in coins, and net bullion flow (in tons) to market from foreign stocks at the New York Federal Reserve Bank.\n",
      "3Includes gold used in the production of consumer purchased bars, coins, and jewelry. Exclu...\n",
      "\n",
      "[2] Date: 2020-01\n",
      "Price: 1560.670...\n"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "query = \"What is the current gold production  ?\"\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "print(\"Answer:\", result[\"result\"])\n",
    "query = \"What is the current gold price  ?\"\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "print(\"Answer:\", result[\"result\"])\n",
    "print(\"\\n--- Source chunks (top-k) ---\")\n",
    "for i, doc in enumerate(result[\"source_documents\"][:2], 1):\n",
    "    print(f\"\\n[{i}] {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745d2fcc",
   "metadata": {},
   "source": [
    "### Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7590e7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def ask(question):\n",
    "    if not question.strip():\n",
    "        return \"Please enter a question.\"\n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    return result[\"result\"]\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=ask,\n",
    "    inputs=gr.Textbox(label=\"Ask about gold & silver\", placeholder=\"e.g. What is US gold production?\"),\n",
    "    outputs=gr.Textbox(label=\"Answer\"),\n",
    "    title=\"Gold & Silver RAG Bot\",\n",
    "    description=\"Ask questions about gold and silver markets. Data from USGS, Silver Institute, World Bank.\",\n",
    ")\n",
    "demo.launch(share=False)  # share=True for public link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40230011",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
